#Hadoop Configurabtion
```
  save:
  conf.set("name", args[2]);
  load:
  context.getConfiguration().get("name")
```
#Hadoop Setup：
- 向hadoop提交job后，hadoop会在MapTask的runNewMapper（）或者runOldMapper（）函数中使用**反馈机制实例化具体的Mapper子类**，然后调用这个对象的run（）函数，其中setup()函数就在这个函数的开始被调用，因为hadoop会向setup（）函数中传递Configuration等一些变量（封装在context中），所以我们可以通过重载setup（）函数来获得系统变量实现自己的功能。

  ```
    /**
   * Expert users can override this method for more complete control over the
   * execution of the Mapper.
   * @param context
   * @throws IOException
   */
   //调用run的时候已经在maptask中。当前task分配到了多个key-val,实例化出具体的Mapper子类,并调用下面的方法
   
  public void run(Context context) throws IOException, InterruptedException {
    setup(context);                     //只运行一次，可以重载实现自己的功能，比如获得Configuration中的参数
    while (context.nextKeyValue()) {
      map(context.getCurrentKey(), context.getCurrentValue(), context);
    }
    cleanup(context);
  }
  ```
  
- Maps are the individual tasks which transform input records into a intermediate records. The transformed intermediate records need not be of the same type as the input records. A given input pair may map to zero or many output pairs.
Map任务是一个转换输入记录到某个中间记录的独立任务，**被转换后的中间记录不需要与输入记录具有相同的类型**。一个给定的输入键值对可能会map到零个或多个输出键值对。

The Hadoop Map-Reduce framework spawns one map task for each InputSplit generated by the InputFormat for the job. Mapperimplementations can access the Configuration for the job via the JobContext.getConfiguration().
InputFormat会为对应的作业产生一个或多个InputSplit，Hadoop **Map-Reduce框架再为每个InputSplit产生一个Map任务**。Mapper的实现类可以通过JobContext.getConfiguration()来获得此作业的Configuration对象（里面保存了各种job的配置信息）。

The framework first calls setup(org.apache.hadoop.mapreduce.Mapper.Context), followed by map(Object, Object, Context) for each key/value pair in the InputSplit. Finally cleanup(Context) is called.
Hadoop Map-Reduce框架首先调用setup(org.apache.hadoop.mapreduce.Mapper.Context)建立工作环境，然后为每个InputSplit中的键值对调用map(Object, Object, Context)函数处理输入数据，**map任务完成后再调用cleanup(Context)做些清理工作**。 
